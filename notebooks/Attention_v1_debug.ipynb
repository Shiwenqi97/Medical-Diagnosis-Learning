{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import random\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import math\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class WordModel(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size, hidden_dim, batch_size):\n",
    "        super(WordModel, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.word_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.word_rnn = nn.GRU(embed_dim, hidden_dim,  bidirectional=True)\n",
    "\n",
    "    def forward(self, x, _hidden):\n",
    "        true_x_size = x.size()\n",
    "        x = x.view(self.batch_size, -1)\n",
    "        #print(\"before embedding\", x.size())\n",
    "        x = self.word_embed(x)\n",
    "        #print(\"after embedding\", x.size())\n",
    "        x = torch.transpose(x, 1, 0)\n",
    "        return self.word_rnn(x, _hidden)\n",
    "\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden1 = Variable(torch.zeros(2, self.batch_size,  self.hidden_dim))\n",
    "        #hidden2 = Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        return hidden1#, hidden2)\n",
    "\n",
    "\n",
    "class Attend(nn.Module):\n",
    "    def __init__(self, batch_size, hidden_dim):\n",
    "        super(Attend, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.lin = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.context = Variable(torch.FloatTensor(hidden_dim))\n",
    "        stdv = 1. / math.sqrt(self.context.size(0))\n",
    "        self.context.data.uniform_(-stdv, stdv)\n",
    "\n",
    "        self.sm = nn.Softmax()\n",
    "    def forward(self, x, sentence_size):\n",
    "        attends = []\n",
    "        for i in range(x.size(0)):\n",
    "            #print(x[i,:,:].size())\n",
    "            attends.append(F.tanh(self.lin(x[i,:,:])).unsqueeze(0))\n",
    "        #print (\"single attend:\", attends[0].size())\n",
    "        attends = torch.cat(attends)\n",
    "        #print(\"cat attention:\", attends.size())\n",
    "        attn_combine = torch.mul(attends, self.context)\n",
    "        #print(\"attention_combine:\", attn_combine.size())\n",
    "        alpha = self.sm(attn_combine.contiguous().view(-1, self.hidden_dim))\n",
    "        #print(\"sm size:\", alpha.size())\n",
    "        #print(x.size())\n",
    "        attended = torch.mul(x, alpha).contiguous().view(self.batch_size, sentence_size, -1, self.hidden_dim)\n",
    "        #print(\"x.alpha prod:\", attended.size())\n",
    "        attended = torch.sum(attended, 2)\n",
    "        #print(\"attended sum:\", attended.size())\n",
    "        return attended\n",
    "\n",
    "class SentModel(nn.Module):\n",
    "    def __init__(self, batch_size, hidden_dim):\n",
    "        super(SentModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.sent_rnn = nn.GRU(hidden_dim, hidden_dim,  bidirectional=True)\n",
    "    def forward(self, x, _hidden):\n",
    "        x = torch.transpose(x, 1, 0)\n",
    "        return self.sent_rnn(x, _hidden)\n",
    "    def init_hidden(self):\n",
    "        hidden1 = Variable(torch.zeros(2, self.batch_size,  self.hidden_dim))\n",
    "        #hidden2 = Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        return hidden1#, hidden2)\n",
    "\n",
    "class Classifer(nn.Module):\n",
    "    def __init__(self, hidden_dim, op_dim):\n",
    "        super(Classifer, self).__init__()\n",
    "        self.lin = nn.Linear(hidden_dim, op_dim)\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "\n",
    "class Ensemble(nn.Module):\n",
    "    def __init__(self, embed_dim, vocabulary_size, hidden_dim, batch_size, label_map):\n",
    "        super(Ensemble, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_rnn = WordModel(embed_dim, vocabulary_size, hidden_dim, batch_size)\n",
    "        self.wordattention = Attend(batch_size, 2*hidden_dim)\n",
    "        self.sent_rnn = SentModel(batch_size, 2*hidden_dim)\n",
    "        self.sentattention = Attend(batch_size, 4*hidden_dim)\n",
    "        self.clf = Classifer(4*hidden_dim, len(label_map.keys()))\n",
    "\n",
    "    def forward(self, batch_x, word_hidden, sent_hidden):\n",
    "        #print(\"raw size:\", batch_x.size())\n",
    "        x, hidden = self.word_rnn(batch_x, word_hidden)\n",
    "        #print(\"word rnn op size:\", x.size())\n",
    "        #print(\"word rnn hidden size:\", hidden.size())\n",
    "        x = x.contiguous().view(batch_x.size(2), batch_x.size(0)*batch_x.size(1), -1) # sent_size x batch_size x 2*hidd\n",
    "        #print(\"============\")\n",
    "        #print(\"word attention ip size:\", x.size())\n",
    "        sentence_reprs = self.wordattention(x, batch_x.size(1)) # batch_size x sent_size x 2*hidden\n",
    "        #print(sentence_reprs.size())\n",
    "        #print(\"============\")\n",
    "        sent_op, sent_hidden = self.sent_rnn(sentence_reprs, sent_hidden)\n",
    "        #print(\"sent rnn op size:\", sent_op.size())\n",
    "        sent_op = sent_op.contiguous().view(batch_x.size(1), self.batch_size, -1) # sent_size x batch_size x 2*hidden\n",
    "        sent_att = self.sentattention(sent_op, 1)\n",
    "        sent_att = sent_att.contiguous().view(self.batch_size, 4*self.hidden_dim)\n",
    "        pred_prob = self.clf(sent_att)\n",
    "        return pred_prob\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import random\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorboard_logger\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "sys.path.append('/home/ag4508/Medical-Diagnosis-Learning/src')\n",
    "from attention_databuilder import *\n",
    "#from attention_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_path = '../data/top50_labels1.csv'\n",
    "log_path = '/home/ag4508/nlp_log'\n",
    "traindata_path = '/misc/vlgscratch2/LecunGroup/anant/nlp/processed_data/50codesL3_UNK_content_4_train_data.pkl'\n",
    "valdata_path = '/misc/vlgscratch2/LecunGroup/anant/nlp/processed_data/50codesL3_UNK_content_4_valid_data.pkl'\n",
    "\n",
    "PADDING = \"<PAD>\"\n",
    "UNKNOWN = \"UNK\"\n",
    "batch_size = 4\n",
    "num_workers = 4\n",
    "embed_dim = 50\n",
    "hidden_dim = 100\n",
    "lr = 1e-2\n",
    "num_epochs = 10\n",
    "log_interval = 1\n",
    "gpu_id = 3\n",
    "_t = time.time()\n",
    "\n",
    "torch.manual_seed(1)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    torch.cuda.set_device(gpu_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data reader\n",
    "# tensorboard_logger.configure(log_path)\n",
    "# traindata = pickle.load(open(traindata_path, 'r'))\n",
    "# valdata = pickle.load(open(valdata_path, 'r'))\n",
    "\n",
    "\n",
    "label_map = {i:_ for _,i in enumerate(get_labels(traindata))}\n",
    "##TODO confirm if padding gets 0th index\n",
    "vocabulary, token2idx  = build_vocab(traindata, PADDING)\n",
    "\n",
    "traindata = traindata[:1000]\n",
    "valdata = valdata[:100]\n",
    "\n",
    "trainset = NotesData(traindata, token2idx, UNKNOWN, label_map)\n",
    "valset = NotesData(valdata, token2idx, UNKNOWN, label_map)\n",
    "print(\"Data Loaded in %.2f mns.\"%((time.time()-_t)/60))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = trainset, batch_size=batch_size, shuffle=True,\n",
    "                                                           num_workers=num_workers, collate_fn=sent_batch_collate)\n",
    "val_loader = torch.utils.data.DataLoader(dataset = valset, batch_size=batch_size, shuffle=True,\n",
    "                                                           num_workers=num_workers, collate_fn=sent_batch_collate)\n",
    "print(\"data loader done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ensemble(embed_dim, len(vocabulary), hidden_dim, batch_size, label_map)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "opti = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "    crit.cuda()\n",
    "    model.wordattention.context = model.wordattention.context.cuda()\n",
    "    model.sentattention.context = model.sentattention.context.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "step = 0\n",
    "train_loss_mean = []\n",
    "for n_e in range(num_epochs):\n",
    "    word_hidden = model.word_rnn.init_hidden()\n",
    "    sent_hidden = model.sent_rnn.init_hidden()\n",
    "    if use_cuda:\n",
    "        word_hidden, sent_hidden = word_hidden.cuda(), sent_hidden.cuda()\n",
    "\n",
    "    for batch in train_loader:\n",
    "        if batch[0].size(0) != batch_size:\n",
    "            continue\n",
    "\n",
    "        model.zero_grad()\n",
    "        batch_x = Variable(batch[0])\n",
    "        batch_y = Variable(batch[1])\n",
    "\n",
    "        if use_cuda:\n",
    "            batch_x, batch_y = batch_x.cuda(), batch_y.cuda()\n",
    "\n",
    "        pred_prob = model(batch_x, word_hidden, sent_hidden)\n",
    "        loss = crit(pred_prob, batch_y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 1.0)\n",
    "        opti.step()\n",
    "\n",
    "        train_loss_mean.append(loss.data[0])\n",
    "        if step % log_interval ==0:\n",
    "            val_loss_mean = 0\n",
    "            word_hidden = model.word_rnn.init_hidden()\n",
    "            sent_hidden = model.sent_rnn.init_hidden()\n",
    "            if use_cuda:\n",
    "                word_hidden, sent_hidden = word_hidden.cuda(), sent_hidden.cuda()\n",
    "\n",
    "            correct = 0\n",
    "            for val_batch in val_loader:\n",
    "                if batch[0].size(0) != batch_size:\n",
    "                    continue\n",
    "\n",
    "                batch_x, batch_y = Variable(batch[0], volatile=True), Variable(batch[1])\n",
    "                if use_cuda:\n",
    "                    batch_x, batch_y = batch_x.cuda(), batch_y.cuda()\n",
    "\n",
    "                outputs = model(batch_x, word_hidden, sent_hidden)\n",
    "                val_loss = crit(outputs, batch_y)\n",
    "                val_loss_mean += val_loss.data[0]\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                correct += predicted.eq(batch_y.data).cpu().sum()\n",
    "\n",
    "            train_loss_mean = np.mean(train_loss_mean)\n",
    "            correct /= float(len(val_loader.dataset))\n",
    "\n",
    "            val_loss_mean /= float(len(val_loader.dataset))\n",
    "            print(\"Epoch: %d, Step: %d, Train Loss: %.2f, Val Loss: %.2f, Val acc: %.2f\"%(n_e, step, train_loss_mean, val_loss_mean, correct))\n",
    "\n",
    "            param1, grad1 = calc_grad_norm(model.parameters(), 1)\n",
    "            param2, grad2 = calc_grad_norm(model.parameters(), 2)\n",
    "            print(\"Param Norm1: %.2f, grad Norm1: %.2f, Param Norm12: %.2f, grad Norm2: %.2f\"%(param1, grad1, param2, grad2))\n",
    "\n",
    "            tensorboard_logger.log_value('train_loss', train_loss_mean, step)\n",
    "            tensorboard_logger.log_value('val_loss', val_loss_mean, step)\n",
    "            tensorboard_logger.log_value('val_acc', correct, step)\n",
    "            tensorboard_logger.log_value('param norm1', param1, step)\n",
    "            tensorboard_logger.log_value('grad norm1', grad1, step)\n",
    "            train_loss_mean = []            \n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_w(m):\n",
    "    torch.nn.init.xavier_uniform(m.weight)\n",
    "model.apply(init_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in model.parameters():\n",
    "    if len(i.size()):\n",
    "        print(i[0, :10])\n",
    "    els\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

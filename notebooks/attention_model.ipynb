{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import random\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "from spacy.lang.en import English\n",
    "tokenizer = English().Defaults.create_tokenizer(nlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "notedata = []\n",
    "with open('../data/mimic/NOTEEVENTS.csv', 'r') as f:\n",
    "    rea = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "    _ = next(rea)\n",
    "    for _,row in enumerate(rea):\n",
    "        if row[6].lower() == \"discharge summary\":\n",
    "            notedata.append([re.sub(\"\\d\", \"d\", row[-1]), row[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_labels(path):\n",
    "    labels = [row[0] for row in  csv.reader(open(path, \"r\"), delimiter=\",\")]\n",
    "    return {i:_ for _,i in enumerate(labels)}\n",
    "\n",
    "def load_summaries(path):\n",
    "    data = []\n",
    "    for row in  csv.reader(open(path, \"r\"), delimiter=\",\", quotechar='\"'):\n",
    "        if row[1] == '':\n",
    "            continue\n",
    "        data.append({\n",
    "            'text': re.sub(\"\\d\", \"d\", row[1]),\n",
    "            'label': row[2].split(',')\n",
    "        })\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_dictionary(data, PADDING, UNKNOWN, vocab_threshold, tokenizer):    \n",
    "    all_tokens = []\n",
    "    for doc in tokenizer.pipe([_['text'] for _ in data], batch_size=50):\n",
    "        all_tokens.extend([str(_) for _ in list(doc)])\n",
    "    word_counter = Counter(all_tokens)\n",
    "    vocabulary = list(set([word for word in word_counter if word_counter[word] > vocab_threshold]))\n",
    "    vocabulary = [PADDING, UNKNOWN] + vocabulary\n",
    "\n",
    "    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "    return (word_indices, vocabulary)\n",
    "\n",
    "class TextData(data.Dataset):\n",
    "    def __init__(self, data, token2idx, UNKNOWN, label_map):\n",
    "        super(TextData, self).__init__()\n",
    "        for i, row in enumerate(data):\n",
    "            tokens = nlp(row['text'])            \n",
    "            sentences = [str(s).replace(\"\\n\", \"\") for s in tokens.sents]                                        \n",
    "            data[i]['text_index_sequence'] = [[token2idx.get(str(word), token2idx[UNKNOWN]) for word in\n",
    "                                                           list(tokenizer(sent))] for sent in sentences]                                    \n",
    "            label_onehot = np.zeros((len(label_map.keys())))\n",
    "            for la in row['label']:\n",
    "                label_onehot[label_map[la]] = 1\n",
    "            data[i]['label'] = label_onehot        \n",
    "\n",
    "        self.data = data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.data[index]['text_index_sequence'], self.data[index]['label'])\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def sent_batch_collate(batch):\n",
    "    max_note_len = max([len(_[0]) for _ in batch])\n",
    "    max_sentence_len = max([len(i) for _ in batch for i in _[0]])\n",
    "    \n",
    "    x = torch.zeros(len(batch), max_note_len, max_sentence_len)\n",
    "    for n,note in enumerate(batch):\n",
    "        for s,sentence in enumerate(note[0]):\n",
    "            for w, word in enumerate(sentence):                                \n",
    "                x[n, s, w] = float(word)\n",
    "\n",
    "    return (x.long(), torch.from_numpy(np.array([_[1] for _ in batch])).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_path = '../data/top50_labels.csv'\n",
    "label_map = {i:_ for _,i in enumerate(get_top_labels(label_path))}\n",
    "\n",
    "data_path = '../data/summaries_labels.csv'\n",
    "training_set = load_summaries(data_path)\n",
    "\n",
    "random.shuffle(training_set)\n",
    "testset = training_set[:int(len(training_set)*0.9)]\n",
    "training_set = training_set[int(len(training_set)*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PADDING = \"<PAD>\"\n",
    "UNKNOWN = \"<UNK>\"\n",
    "max_seq_length = 500\n",
    "min_vocab_threshold = 5\n",
    "batch_size = 1\n",
    "num_workers = 1\n",
    "embed_dim = 50\n",
    "hidden_dim = 100\n",
    "lr = 1e-3\n",
    "num_epochs = 1#00\n",
    "\n",
    "\n",
    "token2idx, vocabulary = build_dictionary(training_set, PADDING, UNKNOWN, min_vocab_threshold, tokenizer)\n",
    "dataset= TextData(training_set[:100], token2idx, UNKNOWN, label_map)\n",
    "train_loader = torch.utils.data.DataLoader(dataset= dataset, batch_size=batch_size, shuffle=True,\n",
    "                                                           num_workers=num_workers, collate_fn=sent_batch_collate)\n",
    "# val_loader = torch.utils.data.DataLoader(dataset= TextData(testset, token2idx, UNKNOWN, label_map), batch_size=batch_size, shuffle=True,\n",
    "#                                                            num_workers=num_workers, collate_fn=sent_batch_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class WordModel(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size, hidden_dim, batch_size):\n",
    "        super(WordModel, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.word_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.word_rnn = nn.GRU(embed_dim, hidden_dim,  bidirectional=True)\n",
    "                \n",
    "    def forward(self, x, _hidden):\n",
    "        true_x_size = x.size()\n",
    "        x = x.view(self.batch_size, -1)\n",
    "        #print(\"before embedding\", x.size())        \n",
    "        x = self.word_embed(x)        \n",
    "        #print(\"after embedding\", x.size())\n",
    "        x = torch.transpose(x, 1, 0)\n",
    "        return self.word_rnn(x, _hidden)\n",
    "        \n",
    "#         descriptors = []#torch.zeros(x.size(-2), _hidden[0].size(0)*2, _hidden[0].size(1), _hidden[0].size(2))\n",
    "#         for w in range(x.size(0)):\n",
    "#             #print(\"seq size:\", x[w, :, :].size())\n",
    "#             op, _hidden  = self.word_rnn(x[w, :, :].contiguous().view(1, self.batch_size, -1), _hidden)\n",
    "#             descriptors.append(torch.cat([_hidden[0], _hidden[1]], 2))\n",
    "            \n",
    "#         descriptors = torch.cat(descriptors, 0)\n",
    "#         #descriptors = torch.transpose(descriptors, 1, 0)#.contiguous().view(batch_size, -1, x.size(0),2*hidden_dim)\n",
    "        \n",
    "#         #x = x[-1, :, :].view(self.batch_size, -1)        \n",
    "#         #print(\"desc size:\", descriptors.size())\n",
    "#         print(\"op full size:\", op.size())\n",
    "#         return (op, descriptors)#[-1, :, :]\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        hidden1 = Variable(torch.zeros(2, self.batch_size,  self.hidden_dim))\n",
    "        #hidden2 = Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        return hidden1#, hidden2)\n",
    "\n",
    "    \n",
    "class Attend(nn.Module):\n",
    "    def __init__(self, batch_size, hidden_dim):\n",
    "        super(Attend, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.lin = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # TODO intialize this!!\n",
    "        self.context = Variable(torch.ones(hidden_dim).float())\n",
    "        self.sm = nn.Softmax()\n",
    "    def forward(self, x, sentence_size):\n",
    "        attends = []\n",
    "        for i in range(x.size(0)):\n",
    "            #print(x[i,:,:].size())\n",
    "            attends.append(F.tanh(self.lin(x[i,:,:])).unsqueeze(0))\n",
    "        #print (\"single attend:\", attends[0].size())\n",
    "        attends = torch.cat(attends)\n",
    "        #print(\"cat attention:\", attends.size())\n",
    "        attn_combine = torch.mul(attends, self.context)\n",
    "        #print(\"attention_combine:\", attn_combine.size())        \n",
    "        alpha = self.sm(attn_combine.contiguous().view(-1, self.hidden_dim))\n",
    "        #print(\"sm size:\", alpha.size())\n",
    "        #print(x.size())\n",
    "        attended = torch.mul(x, alpha).contiguous().view(self.batch_size, sentence_size, -1, self.hidden_dim)\n",
    "        #print(\"x.alpha prod:\", attended.size())\n",
    "        attended = torch.sum(attended, 2)\n",
    "        #print(\"attended sum:\", attended.size())\n",
    "        return attended\n",
    "\n",
    "class SentModel(nn.Module):\n",
    "    def __init__(self, batch_size, hidden_dim):\n",
    "        super(SentModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.sent_rnn = nn.GRU(hidden_dim, hidden_dim,  bidirectional=True)\n",
    "    def forward(self, x, _hidden):\n",
    "        x = torch.transpose(x, 1, 0)\n",
    "        return self.sent_rnn(x, _hidden)\n",
    "    def init_hidden(self):\n",
    "        hidden1 = Variable(torch.zeros(2, self.batch_size,  self.hidden_dim))\n",
    "        #hidden2 = Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        return hidden1#, hidden2)      \n",
    "\n",
    "class Classifer(nn.Module):\n",
    "    def __init__(self, hidden_dim, op_dim):\n",
    "        super(Classifer, self).__init__()\n",
    "        self.lin = nn.Linear(hidden_dim, op_dim)\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = WordModel(embed_dim, len(vocabulary), hidden_dim, batch_size)\n",
    "wordattention = Attend(batch_size, 2*hidden_dim)\n",
    "sent_rnn = SentModel(batch_size, 2*hidden_dim)\n",
    "sentattention = Attend(batch_size, 4*hidden_dim)\n",
    "clf = Classifer(4*hidden_dim, len(label_map.keys()))\n",
    "crit = nn.BCEWithLogitsLoss()\n",
    "all_params = list(model.parameters()) + list(wordattention.parameters()) + \\\n",
    "    list(sent_rnn.parameters()) + list(sentattention.parameters())+ list(clf.parameters())\n",
    "opti = torch.optim.Adam(all_params, lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "for batch in train_loader:\n",
    "    batch_x = Variable(batch[0])\n",
    "    batch_y = Variable(batch[1])\n",
    "    _hidden = model.init_hidden()\n",
    "    #print(\"raw size:\", batch_x.size())\n",
    "    x, hidden = model(batch_x, _hidden)\n",
    "    #print(\"word rnn op size:\", x.size())\n",
    "    #print(\"word rnn hidden size:\", hidden.size())    \n",
    "    x = x.contiguous().view(batch_x.size(2), batch_x.size(0)*batch_x.size(1), -1) # sent_size x batch_size x 2*hidd\n",
    "    #print(\"============\")\n",
    "    #print(\"word attention ip size:\", x.size())\n",
    "    sentence_reprs = wordattention(x, batch_x.size(1)) # batch_size x sent_size x 2*hidden\n",
    "    #print(sentence_reprs.size())\n",
    "    #print(\"============\")    \n",
    "    sent_hidden = sent_rnn.init_hidden()\n",
    "    sent_op, sent_hidden = sent_rnn(sentence_reprs, sent_hidden)\n",
    "    #print(\"sent rnn op size:\", sent_op.size())\n",
    "    sent_op = sent_op.contiguous().view(batch_x.size(1), batch_size, -1) # sent_size x batch_size x 2*hidden\n",
    "    sent_att = sentattention(sent_op, 1)\n",
    "    sent_att = sent_att.contiguous().view(batch_size, 4*hidden_dim)\n",
    "    pred_prob = clf(sent_att)\n",
    "    loss = crit(pred_prob, batch_y)\n",
    "    loss.backward()\n",
    "    opti.step()\n",
    "    print(\"loss:\", loss.data[0])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
